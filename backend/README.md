# Novel Builder Backend

FastAPI backend service for novel crawling and management with multi-site support.

## ğŸš€ Features

- **Multi-site novel crawling** - Support for multiple novel websites
- **Unified API interface** - Consistent response format regardless of source
- **Token-based authentication** - Secure API access
- **Real-time content fetching** - On-demand chapter content retrieval
- **Modern Python project structure** - Using pyproject.toml for dependency management

## ğŸ“‹ Prerequisites

- Python 3.11+
- Docker & Docker Compose (for containerized deployment)
- Git

## ğŸ› ï¸ Development Setup

### Local Development

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd novel-builder/backend
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   # Install production dependencies
   pip install -e .

   # Install development dependencies
   pip install -e ".[dev]"
   ```

4. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your configuration
   ```

5. **Run pre-commit setup**
   ```bash
   pre-commit install
   ```

6. **Run the development server**
   ```bash
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```

### Docker Development

1. **Build and run with Docker Compose**
   ```bash
   docker-compose up --build
   ```

2. **Run tests in Docker**
   ```bash
   docker-compose -f docker-compose.test.yml up --build --abort-on-container-exit
   ```

## ğŸ§ª Testing

### Run All Tests
```bash
pytest
```

### Run Tests with Coverage
```bash
pytest --cov=app --cov-report=html
```

### Run Specific Test File
```bash
pytest tests/test_main.py
```

### Run Tests by Marker
```bash
pytest -m unit        # Unit tests only
pytest -m integration # Integration tests only
```

## ğŸ” Code Quality

### Static Analysis
```bash
# Run all checks
ruff check .          # Fast linting and formatting
pylint app/           # Deep code quality check
mypy app/             # Static type checking

# Format code
ruff format .         # Auto-format with ruff
black .               # Alternative formatter
isort .               # Sort imports
```

### Pre-commit Hooks
The project uses pre-commit hooks to ensure code quality. They will run automatically before each commit.

To run them manually:
```bash
pre-commit run --all-files
```

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app/                    # Application code
â”‚   â”œâ”€â”€ api/               # API route handlers
â”‚   â”œâ”€â”€ core/              # Core application logic
â”‚   â”œâ”€â”€ models/            # Database models
â”‚   â”œâ”€â”€ schemas/           # Pydantic models
â”‚   â”œâ”€â”€ services/          # Business logic services
â”‚   â”œâ”€â”€ deps/              # Dependencies (auth, etc.)
â”‚   â””â”€â”€ main.py           # FastAPI application entry point
â”œâ”€â”€ tests/                 # Test files
â”‚   â”œâ”€â”€ unit/             # Unit tests
â”‚   â”œâ”€â”€ integration/      # Integration tests
â”‚   â””â”€â”€ conftest.py       # Test configuration
â”œâ”€â”€ pyproject.toml         # Project configuration and dependencies
â”œâ”€â”€ Dockerfile            # Production Docker image
â”œâ”€â”€ docker-compose.yml    # Development environment
â”œâ”€â”€ docker-compose.test.yml # Test environment
â”œâ”€â”€ .env.example          # Environment variables template
â”œâ”€â”€ .pre-commit-config.yaml # Pre-commit hooks configuration
â””â”€â”€ README.md             # This file
```

## ğŸ”§ Configuration

### Environment Variables

Key environment variables (see `.env.example`):

- `NOVEL_API_TOKEN`: API authentication token (required)
- `NOVEL_ENABLED_SITES`: Comma-separated list of enabled crawler sites
- `SECRET_KEY`: JWT secret key for authentication
- `DEBUG`: Enable debug mode

### Adding New Crawlers

1. Create a new crawler class in `app/services/crawlers/`
2. Inherit from `BaseCrawler`
3. Implement required methods (`search`, `get_chapters`, `get_chapter_content`)
4. Register in `crawler_factory.py`
5. Add to `NOVEL_ENABLED_SITES` environment variable

## ğŸ“š API Documentation

Once running, visit:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **OpenAPI Schema**: http://localhost:8000/openapi.json

### Authentication

All API endpoints require `X-API-TOKEN` header:
```
X-API-TOKEN: your-api-token-here
```

### Main Endpoints

- `GET /health` - Health check
- `GET /search` - Search novels across enabled sites
- `GET /chapters` - Get chapter list for a novel
- `GET /chapter-content` - Get specific chapter content

## ğŸš€ Deployment

### Production Docker Build
```bash
docker build -t novel-backend .
docker run -p 8000:8000 --env-file .env novel-backend
```

### Using Docker Compose (Production)
```bash
docker-compose -f docker-compose.prod.yml up -d
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and code quality checks
5. Submit a pull request

### Code Style

This project follows:
- **PEP 8** for Python code style
- **Black** for code formatting (line length: 88)
- **Ruff** for fast linting
- **MyPy** for type checking
- **PyLint** for deep code analysis

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ” Troubleshooting

### Common Issues

1. **Import errors**: Make sure you're in the virtual environment
2. **Permission denied**: Check Docker permissions or use `sudo`
3. **Port already in use**: Change port in docker-compose.yml or stop conflicting services
4. **Tests failing**: Check environment variables and dependencies

### Getting Help

- Check the [Issues](../../issues) page
- Read the [Documentation](../../wiki)
- Create a new issue with detailed information