# Novel Builder çˆ¬è™«å¼€å‘è§„èŒƒ

## ğŸ“‹ ç›®å½•
- [æ¶æ„æ¦‚è¿°](#æ¶æ„æ¦‚è¿°)
- [å¼€å‘ç¯å¢ƒè¦æ±‚](#å¼€å‘ç¯å¢ƒè¦æ±‚)
- [çˆ¬è™«å‘½åè§„èŒƒ](#çˆ¬è™«å‘½åè§„èŒƒ)
- [æ ¸å¿ƒæ¥å£è§„èŒƒ](#æ ¸å¿ƒæ¥å£è§„èŒƒ)
- [HTTPå®¢æˆ·ç«¯ä½¿ç”¨è§„èŒƒ](#httpå®¢æˆ·ç«¯ä½¿ç”¨è§„èŒƒ)
- [æ•°æ®å¤„ç†è§„èŒƒ](#æ•°æ®å¤„ç†è§„èŒƒ)
- [é”™è¯¯å¤„ç†è§„èŒƒ](#é”™è¯¯å¤„ç†è§„èŒƒ)
- [æµ‹è¯•è§„èŒƒ](#æµ‹è¯•è§„èŒƒ)
- [éƒ¨ç½²å’Œé…ç½®](#éƒ¨ç½²å’Œé…ç½®)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [å¼€å‘æ¨¡æ¿](#å¼€å‘æ¨¡æ¿)

## ğŸ—ï¸ æ¶æ„æ¦‚è¿°

### åˆ†å±‚æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           APIå±‚ (FastAPI)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         æœåŠ¡å±‚ (Services)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  çˆ¬è™«æŠ½è±¡å±‚ (EnhancedBaseCrawler)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    HTTPå®¢æˆ·ç«¯æŠ½è±¡å±‚ (HttpClient)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    ç½‘ç»œå®ç°å±‚ (Requests/Playwright)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒè®¾è®¡åŸåˆ™
- **å•ä¸€èŒè´£**: çˆ¬è™«åªè´Ÿè´£æ•°æ®æå–ï¼Œä¸å¤„ç†ä¸šåŠ¡é€»è¾‘
- **ä¾èµ–æ³¨å…¥**: é€šè¿‡å·¥å‚æ¨¡å¼ç»Ÿä¸€ç®¡ç†çˆ¬è™«å®ä¾‹
- **ç­–ç•¥æ¨¡å¼**: HTTPå®¢æˆ·ç«¯æ”¯æŒå¤šç§è¯·æ±‚ç­–ç•¥
- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰çˆ¬è™«å¿…é¡»å®ç°ç›¸åŒçš„æŠ½è±¡æ¥å£
- **é”™è¯¯éš”ç¦»**: å•ä¸ªçˆ¬è™«æ•…éšœä¸å½±å“æ•´ä½“æœåŠ¡

## ğŸ”§ å¼€å‘ç¯å¢ƒè¦æ±‚

### Pythonç‰ˆæœ¬
- Python 3.8+
- æ¨èä½¿ç”¨ Python 3.10+

### ä¾èµ–åŒ…
```python
# æ ¸å¿ƒä¾èµ–
fastapi>=0.100.0
beautifulsoup4>=4.12.0
requests>=2.31.0
playwright>=1.40.0

# æ•°æ®å¤„ç†
pydantic>=2.0.0
sqlalchemy>=2.0.0

# å·¥å…·åŒ…
lxml>=4.9.0
urllib3>=2.0.0
```

### å¼€å‘å·¥å…·
```bash
# ä»£ç è´¨é‡
ruff          # ä»£ç æ£€æŸ¥å’Œæ ¼å¼åŒ–
mypy          # ç±»å‹æ£€æŸ¥
pytest        # æµ‹è¯•æ¡†æ¶

# Dockeræ”¯æŒ
docker        # å®¹å™¨åŒ–
docker-compose # ç¼–æ’å·¥å…·
```

## ğŸ“ çˆ¬è™«å‘½åè§„èŒƒ

### ç±»åè§„èŒƒ
```python
# æ ¼å¼ï¼š{SiteName}CrawlerRefactored
class AliceSWCrawlerRefactored(EnhancedBaseCrawler):
    """AliceSWç½‘ç«™çˆ¬è™«é‡æ„ç‰ˆ"""

class ShukugeCrawlerRefactored(EnhancedBaseCrawler):
    """ä¹¦åº“ç½‘ç«™çˆ¬è™«é‡æ„ç‰ˆ"""
```

### æ–‡ä»¶åè§„èŒƒ
```bash
# æ ¼å¼ï¼š{site_name}_crawler_refactored.py
alice_sw_crawler_refactored.py
shukuge_crawler_refactored.py
xspsw_crawler_refactored.py
```

### å¸¸é‡å’Œæ–¹æ³•å‘½å
```python
# ç§æœ‰æ–¹æ³•ä½¿ç”¨ä¸‹åˆ’çº¿å‰ç¼€
def _extract_search_results(self, soup, keyword: str):
    """æå–æœç´¢ç»“æœ"""

def _should_skip_link(self, title: str, href: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦è·³è¿‡é“¾æ¥"""

# ç«™ç‚¹ç‰¹å®šæ–¹æ³•ä½¿ç”¨ç«™ç‚¹å‰ç¼€
def _extract_alice_sw_author(self, element):
    """æå–AliceSWä½œè€…ä¿¡æ¯"""

def _parse_shukuge_chapter_list(self, soup):
    """è§£æShukugeç« èŠ‚åˆ—è¡¨"""
```

## ğŸ”Œ æ ¸å¿ƒæ¥å£è§„èŒƒ

### å¿…é¡»å®ç°çš„æŠ½è±¡æ–¹æ³•
```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List

class EnhancedBaseCrawler(ABC):
    @abstractmethod
    async def search_novels(self, keyword: str) -> List[Dict[str, Any]]:
        """
        æœç´¢å°è¯´

        Args:
            keyword: æœç´¢å…³é”®è¯

        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ ¼å¼è§ä¸‹æ–¹è§„èŒƒ

        Raises:
            Exception: æœç´¢å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        pass

    @abstractmethod
    async def get_chapter_list(self, novel_url: str) -> List[Dict[str, Any]]:
        """
        è·å–ç« èŠ‚åˆ—è¡¨

        Args:
            novel_url: å°è¯´è¯¦æƒ…é¡µURL

        Returns:
            List[Dict]: ç« èŠ‚åˆ—è¡¨ï¼Œæ ¼å¼è§ä¸‹æ–¹è§„èŒƒ

        Raises:
            Exception: è·å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        pass

    @abstractmethod
    async def get_chapter_content(self, chapter_url: str) -> Dict[str, Any]:
        """
        è·å–ç« èŠ‚å†…å®¹

        Args:
            chapter_url: ç« èŠ‚URL

        Returns:
            Dict: ç« èŠ‚å†…å®¹ï¼Œæ ¼å¼è§ä¸‹æ–¹è§„èŒƒ

        Raises:
            Exception: è·å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        pass
```

### æ ‡å‡†æ•°æ®æ ¼å¼

#### æœç´¢ç»“æœæ ¼å¼
```python
[
    {
        "title": "å°è¯´æ ‡é¢˜",           # å¿…éœ€
        "author": "ä½œè€…åç§°",         # å¿…éœ€
        "url": "å°è¯´è¯¦æƒ…é¡µURL",      # å¿…éœ€
        "cover_url": "å°é¢å›¾ç‰‡URL",   # å¯é€‰
        "description": "å°è¯´ç®€ä»‹",    # å¯é€‰
        "status": "è¿è½½/å®Œç»“",       # å¯é€‰
        "category": "å°è¯´åˆ†ç±»",      # å¯é€‰
        "last_updated": "æ›´æ–°æ—¶é—´",   # å¯é€‰
        "source": "alice_sw"         # è‡ªåŠ¨æ·»åŠ ï¼Œæ— éœ€æ‰‹åŠ¨è®¾ç½®
    }
]
```

#### ç« èŠ‚åˆ—è¡¨æ ¼å¼
```python
[
    {
        "title": "ç« èŠ‚æ ‡é¢˜",         # å¿…éœ€
        "url": "ç« èŠ‚URL"            # å¿…éœ€
    }
]
```

#### ç« èŠ‚å†…å®¹æ ¼å¼
```python
{
    "title": "ç« èŠ‚æ ‡é¢˜",           # å¿…éœ€
    "content": "ç« èŠ‚æ­£æ–‡å†…å®¹"      # å¿…éœ€
}
```

## ğŸŒ HTTPå®¢æˆ·ç«¯ä½¿ç”¨è§„èŒƒ

### è¯·æ±‚ç­–ç•¥é€‰æ‹©
```python
from .http_client import RequestStrategy

# ç­–ç•¥é€‰æ‹©æŒ‡å—
strategies = {
    "ç®€å•HTTPç½‘ç«™": RequestStrategy.SIMPLE,      # ä½¿ç”¨requests
    "å¤æ‚SPAç½‘ç«™": RequestStrategy.BROWSER,      # ä½¿ç”¨playwright
    "éœ€è¦åçˆ¬è™«": RequestStrategy.HYBRID,        # æ··åˆæ¨¡å¼ï¼Œä¼˜å…ˆrequests
    "é«˜çº§åæ£€æµ‹": RequestStrategy.STEALTH         # éšè”½æ¨¡å¼
}
```

### çˆ¬è™«åˆå§‹åŒ–
```python
class ExampleCrawler(EnhancedBaseCrawler):
    def __init__(self):
        super().__init__(
            base_url="https://www.example.com",
            strategy=RequestStrategy.HYBRID  # æ ¹æ®ç½‘ç«™ç‰¹æ€§é€‰æ‹©ç­–ç•¥
        )

        # è‡ªå®šä¹‰è¯·æ±‚å¤´
        self.custom_headers = {
            "User-Agent": "Mozilla/5.0...",
            "Accept": "text/html...",
            "Accept-Language": "zh-CN,zh;q=0.9"
        }

        # æµè§ˆå™¨å‚æ•°ï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
        self.browser_args = [
            '--disable-web-security',
            '--no-sandbox'
        ]
```

### è¯·æ±‚é…ç½®è§„èŒƒ
```python
async def search_novels(self, keyword: str):
    try:
        # æ ‡å‡†é…ç½®
        config = RequestConfig(
            timeout=15,              # è¶…æ—¶æ—¶é—´
            max_retries=3,           # æœ€å¤§é‡è¯•æ¬¡æ•°
            strategy=self.strategy,   # è¯·æ±‚ç­–ç•¥
            custom_headers=self.custom_headers,  # è‡ªå®šä¹‰è¯·æ±‚å¤´
            verify_ssl=False,        # SSLéªŒè¯ï¼ˆé’ˆå¯¹é—®é¢˜è¯ä¹¦ï¼‰
            browser_args=self.browser_args  # æµè§ˆå™¨å‚æ•°
        )

        # å‘é€è¯·æ±‚
        response = await self.get_page(url, timeout=15)

        # æˆ–è€…POSTè¯·æ±‚
        response = await self.post_form(url, data, timeout=15)

    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        return []
```

### å“åº”å¤„ç†è§„èŒƒ
```python
# è·å–BeautifulSoupå¯¹è±¡
soup = response.soup()

# è·å–åŸå§‹å†…å®¹
content = response.content

# è·å–ç¼–ç ä¿¡æ¯
encoding = response.encoding

# æ£€æŸ¥è¯·æ±‚ç­–ç•¥
strategy_used = response.strategy_used

# æ£€æŸ¥æ˜¯å¦æ¥è‡ªç¼“å­˜
from_cache = response.from_cache
```

## ğŸ“Š æ•°æ®å¤„ç†è§„èŒƒ

### HTMLè§£æè§„èŒƒ
```python
# ä½¿ç”¨CSSé€‰æ‹©å™¨ä¼˜å…ˆäºæ­£åˆ™è¡¨è¾¾å¼
title_element = soup.select_one('h1.title, .book-title, #book-title')
if title_element:
    title = title_element.get_text().strip()

# å¤šé€‰æ‹©å™¨é™çº§ç­–ç•¥
content_selectors = [
    '#content',
    '.content',
    '.chapter-content',
    'div[class*="content"]'
]

content = None
for selector in content_selectors:
    content = soup.select_one(selector)
    if content:
        break
```

### æ•°æ®æ¸…ç†è§„èŒƒ
```python
# ä½¿ç”¨åŸºç±»æä¾›çš„æ¸…ç†æ–¹æ³•
cleaned_text = self.clean_text(raw_text)

# è‡ªå®šä¹‰æ¸…ç†ï¼ˆåœ¨åŸºç±»æ–¹æ³•åŸºç¡€ä¸Šï¼‰
def clean_novel_content(self, text: str) -> str:
    # å…ˆä½¿ç”¨åŸºç±»æ¸…ç†
    text = self.clean_text(text)

    # ç«™ç‚¹ç‰¹å®šæ¸…ç†
    text = re.sub(r'è¯·è®°ä½æœ¬ç«™åŸŸå.*$', '', text)
    text = re.sub(r'æœ€æ–°ç« èŠ‚.*$', '', text)

    return text.strip()
```

### URLå¤„ç†è§„èŒƒ
```python
import urllib.parse

# URLæ‹¼æ¥
full_url = urllib.parse.urljoin(self.base_url, relative_url)

# URLå‚æ•°ç¼–ç 
params = {'key': keyword, 'type': 'all'}
full_url = f"{search_url}?{urllib.parse.urlencode(params)}"

# URLæœ‰æ•ˆæ€§æ£€æŸ¥
if not href or href.startswith(('javascript:', '#', 'mailto:')):
    continue
```

### æ•°æ®éªŒè¯è§„èŒƒ
```python
# æœç´¢ç»“æœéªŒè¯
def _validate_novel_info(self, novel_info: Dict[str, Any]) -> bool:
    """éªŒè¯å°è¯´ä¿¡æ¯å®Œæ•´æ€§"""
    required_fields = ['title', 'author', 'url']

    for field in required_fields:
        if not novel_info.get(field) or len(novel_info[field].strip()) < 2:
            return False

    # URLæ ¼å¼æ£€æŸ¥
    if not novel_info['url'].startswith(('http://', 'https://')):
        return False

    return True

# ç« èŠ‚éªŒè¯
def _validate_chapter(self, chapter: Dict[str, Any]) -> bool:
    """éªŒè¯ç« èŠ‚ä¿¡æ¯"""
    title = chapter.get('title', '').strip()
    url = chapter.get('url', '').strip()

    return len(title) > 1 and url.startswith('http')
```

## âš ï¸ é”™è¯¯å¤„ç†è§„èŒƒ

### å¼‚å¸¸å¤„ç†åŸåˆ™
```python
async def search_novels(self, keyword: str):
    try:
        # ä¸»è¦é€»è¾‘
        results = await self._perform_search(keyword)
        return results

    except requests.exceptions.Timeout:
        print(f"æœç´¢è¶…æ—¶: {keyword}")
        return []

    except requests.exceptions.ConnectionError:
        print(f"ç½‘ç»œè¿æ¥å¤±è´¥: {keyword}")
        return []

    except Exception as e:
        print(f"æœç´¢å¤±è´¥: {keyword}, é”™è¯¯: {str(e)}")
        return []
```

### é‡è¯•æœºåˆ¶
```python
async def _get_with_retry(self, url: str, max_retries: int = 3):
    """å¸¦é‡è¯•çš„è¯·æ±‚"""
    for attempt in range(max_retries):
        try:
            return await self.get_page(url)

        except Exception as e:
            if attempt == max_retries - 1:
                raise e

            # æŒ‡æ•°é€€é¿
            delay = 2 ** attempt
            await asyncio.sleep(delay)
```

### ä¼˜é›…é™çº§
```python
async def get_chapter_list(self, novel_url: str):
    try:
        # ä¸»è¦æ–¹æ³•
        chapters = await self._extract_from_detail_page(novel_url)
        if chapters:
            return chapters

    except Exception:
        pass

    try:
        # å¤‡ç”¨æ–¹æ³•
        chapters = await self._extract_from_reading_page(novel_url)
        if chapters:
            return chapters

    except Exception:
        pass

    # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
    return await self._extract_generic_chapters(novel_url)
```

## ğŸ§ª æµ‹è¯•è§„èŒƒ

### å•å…ƒæµ‹è¯•ç»“æ„
```python
import pytest
import asyncio
from app.services.alice_sw_crawler_refactored import AliceSWCrawlerRefactored

class TestAliceSWCrawler:
    @pytest.fixture
    def crawler(self):
        return AliceSWCrawlerRefactored()

    @pytest.mark.asyncio
    async def test_search_novels(self, crawler):
        """æµ‹è¯•æœç´¢åŠŸèƒ½"""
        results = await crawler.search_novels("æ–—ç ´è‹ç©¹")

        assert isinstance(results, list)
        if results:  # å¦‚æœæœ‰ç»“æœ
            assert all(isinstance(r, dict) for r in results)
            assert all('title' in r and 'url' in r for r in results)

    @pytest.mark.asyncio
    async def test_get_chapter_list(self, crawler):
        """æµ‹è¯•ç« èŠ‚åˆ—è¡¨è·å–"""
        # å…ˆè·å–ä¸€ä¸ªå°è¯´URL
        search_results = await crawler.search_novels("test")
        if search_results:
            novel_url = search_results[0]['url']
            chapters = await crawler.get_chapter_list(novel_url)

            assert isinstance(chapters, list)
            assert all('title' in c and 'url' in c for c in chapters)
```

### é›†æˆæµ‹è¯•
```python
@pytest.mark.asyncio
async def test_complete_workflow():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
    crawler = AliceSWCrawlerRefactored()

    # 1. æœç´¢
    novels = await crawler.search_novels("æµ‹è¯•å°è¯´")
    assert len(novels) > 0

    # 2. è·å–ç« èŠ‚åˆ—è¡¨
    novel_url = novels[0]['url']
    chapters = await crawler.get_chapter_list(novel_url)
    assert len(chapters) > 0

    # 3. è·å–ç« èŠ‚å†…å®¹
    chapter_url = chapters[0]['url']
    content = await crawler.get_chapter_content(chapter_url)
    assert 'title' in content
    assert len(content['content']) > 100  # å†…å®¹é•¿åº¦æ£€æŸ¥
```

## ğŸš€ éƒ¨ç½²å’Œé…ç½®

### ç¯å¢ƒå˜é‡é…ç½®
```bash
# å¯ç”¨çˆ¬è™«ç«™ç‚¹
NOVEL_ENABLED_SITES=alice_sw,shukuge,xspsw

# APIè®¤è¯
NOVEL_API_TOKEN=your-secret-token

# ä»£ç†é…ç½®ï¼ˆå¯é€‰ï¼‰
HTTP_PROXY=http://proxy.example.com:7890
HTTPS_PROXY=http://proxy.example.com:7890

# è°ƒè¯•æ¨¡å¼
DEBUG=true
```

### çˆ¬è™«å·¥å‚æ³¨å†Œ
```python
# åœ¨ crawler_factory.py ä¸­æ³¨å†Œæ–°çˆ¬è™«
SOURCE_SITES_METADATA = {
    "new_site": {
        "name": "æ–°å°è¯´ç½‘ç«™",
        "base_url": "https://www.newsite.com",
        "description": "ç½‘ç«™æè¿°",
        "search_enabled": True,
        "crawler_class": NewSiteCrawlerRefactored
    }
}

def get_enabled_crawlers():
    enabled = os.getenv("NOVEL_ENABLED_SITES", "").lower()
    crawlers = {}

    if not enabled or "new_site" in enabled:
        crawlers["new_site"] = NewSiteCrawlerRefactored()

    return crawlers
```

### Dockeréƒ¨ç½²
```dockerfile
# ç¡®ä¿å®‰è£…æ‰€æœ‰ä¾èµ–
RUN pip install playwright
RUN playwright install chromium

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV NOVEL_ENABLED_SITES=alice_sw,shukuge,xspsw,new_site
ENV PYTHONPATH=/app
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–
```python
# ä½¿ç”¨è¿æ¥æ± 
self.custom_headers = {
    "Connection": "keep-alive"
}

# åˆç†è®¾ç½®è¶…æ—¶
config = RequestConfig(timeout=15, max_retries=3)

# ç¼“å­˜åˆ©ç”¨ï¼ˆHTTPå®¢æˆ·ç«¯è‡ªåŠ¨å¤„ç†ï¼‰
response = await self.get_page(url)  # è‡ªåŠ¨ç¼“å­˜å“åº”
```

### 2. åçˆ¬è™«ç­–ç•¥
```python
# éšæœºå»¶è¿Ÿ
import random
await asyncio.sleep(random.uniform(1, 3))

# è½®æ¢User-Agent
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
]
self.custom_headers["User-Agent"] = random.choice(user_agents)

# ä½¿ç”¨æ··åˆç­–ç•¥
strategy = RequestStrategy.HYBRID  # è‡ªåŠ¨é™çº§
```

### 3. æ•°æ®è´¨é‡
```python
# å†…å®¹é•¿åº¦æ£€æŸ¥
if len(content) < 300:
    raise Exception("å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½è·å–å¤±è´¥")

# æ ‡é¢˜è§„èŒƒåŒ–
def normalize_title(self, title: str) -> str:
    title = title.strip()
    title = re.sub(r'^\d+\.\s*', '', title)  # å»é™¤åºå·
    title = re.sub(r'[_\-]{2,}', '', title)   # å»é™¤å¤šä½™ç¬¦å·
    return title

# å»é‡å¤„ç†
def deduplicate_results(self, results: List[Dict]) -> List[Dict]:
    seen = set()
    unique = []
    for item in results:
        key = (item['title'], item['url'])
        if key not in seen:
            unique.append(item)
            seen.add(key)
    return unique
```

### 4. ç›‘æ§å’Œæ—¥å¿—
```python
import logging

logger = logging.getLogger(__name__)

async def search_novels(self, keyword: str):
    logger.info(f"å¼€å§‹æœç´¢: {keyword}")
    start_time = time.time()

    try:
        results = await self._perform_search(keyword)
        elapsed = time.time() - start_time

        logger.info(f"æœç´¢å®Œæˆ: {keyword}, æ‰¾åˆ° {len(results)} ä¸ªç»“æœ, è€—æ—¶ {elapsed:.2f}s")
        return results

    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {keyword}, é”™è¯¯: {str(e)}")
        return []
```

## ğŸ“‹ å¼€å‘æ¨¡æ¿

### æ–°çˆ¬è™«æ¨¡æ¿
```python
#!/usr/bin/env python3
"""
{SiteName}çˆ¬è™«

ç½‘ç«™æè¿°ã€ç‰¹æ€§è¯´æ˜
"""

import re
import urllib.parse
from typing import Any, Dict, List

from .enhanced_base_crawler import EnhancedBaseCrawler
from .http_client import RequestConfig, RequestStrategy


class {SiteName}CrawlerRefactored(EnhancedBaseCrawler):
    """{SiteName}ç½‘ç«™çˆ¬è™«"""

    def __init__(self):
        super().__init__(
            base_url="https://www.{site_name}.com",
            strategy=RequestStrategy.{STRATEGY}
        )

        # è‡ªå®šä¹‰è¯·æ±‚å¤´
        self.custom_headers = {
            "User-Agent": "Mozilla/5.0...",
            "Accept": "text/html...",
            "Accept-Language": "zh-CN,zh;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive"
        }

        # æµè§ˆå™¨å‚æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self.browser_args = [
            '--no-sandbox',
            '--disable-web-security'
        ]

    async def search_novels(self, keyword: str) -> List[Dict[str, Any]]:
        """æœç´¢å°è¯´"""
        try:
            # æœç´¢URL
            search_url = f"{self.base_url}/search"

            # è¯·æ±‚é…ç½®
            config = RequestConfig(
                timeout=15,
                max_retries=3,
                strategy=self.strategy,
                custom_headers=self.custom_headers
            )

            # æœç´¢å‚æ•°ï¼ˆæ ¹æ®ç½‘ç«™è°ƒæ•´ï¼‰
            search_data = {
                "keyword": keyword.strip()
            }

            # å‘é€è¯·æ±‚
            response = await self.post_form(search_url, search_data, timeout=15)

            # æå–æœç´¢ç»“æœ
            novels = self._extract_search_results(response.soup(), keyword)

            # æ•°æ®éªŒè¯å’Œæ¸…ç†
            valid_novels = []
            for novel in novels:
                if self._validate_novel_info(novel):
                    valid_novels.append(novel)

            # å»é‡
            return self._deduplicate_results(valid_novels)[:20]

        except Exception as e:
            print(f"{self.__class__.__name__}æœç´¢å¤±è´¥: {str(e)}")
            return []

    async def get_chapter_list(self, novel_url: str) -> List[Dict[str, Any]]:
        """è·å–ç« èŠ‚åˆ—è¡¨"""
        try:
            # è·å–å°è¯´è¯¦æƒ…é¡µ
            response = await self.get_page(novel_url, timeout=15)

            # æå–ç« èŠ‚åˆ—è¡¨
            chapters = self._extract_chapter_list(response.soup(), novel_url)

            # æ•°æ®éªŒè¯
            valid_chapters = []
            for chapter in chapters:
                if self._validate_chapter(chapter):
                    valid_chapters.append(chapter)

            return valid_chapters

        except Exception as e:
            print(f"{self.__class__.__name__}è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    async def get_chapter_content(self, chapter_url: str) -> Dict[str, Any]:
        """è·å–ç« èŠ‚å†…å®¹"""
        try:
            # è·å–ç« èŠ‚é¡µé¢
            response = await self.get_page(chapter_url, timeout=15)
            soup = response.soup()

            # æå–æ ‡é¢˜
            title = self._extract_chapter_title(soup)

            # æå–å†…å®¹
            content = self._extract_chapter_content(soup)

            # å†…å®¹æ¸…ç†
            content = self.clean_novel_content(content)

            return {
                "title": title,
                "content": content
            }

        except Exception as e:
            print(f"{self.__class__.__name__}è·å–ç« èŠ‚å†…å®¹å¤±è´¥: {str(e)}")
            return {
                "title": "ç« èŠ‚å†…å®¹",
                "content": f"è·å–å¤±è´¥: {str(e)}"
            }

    # ==================== ç«™ç‚¹ç‰¹å®šæ–¹æ³• ====================

    def _extract_search_results(self, soup, keyword: str) -> List[Dict[str, Any]]:
        """æå–æœç´¢ç»“æœ"""
        novels = []

        # æ ¹æ®ç½‘ç«™ç»“æ„ç¼–å†™é€‰æ‹©å™¨
        result_items = soup.find_all("div", class_="result-item")

        for item in result_items:
            try:
                # æå–æ ‡é¢˜å’Œé“¾æ¥
                title_link = item.find("a", href=True)
                if not title_link:
                    continue

                title = title_link.get_text().strip()
                href = title_link.get("href", "")
                full_url = urllib.parse.urljoin(self.base_url, href)

                # æå–ä½œè€…
                author = self._extract_author(item)

                # æå–å…¶ä»–ä¿¡æ¯
                novel_info = {
                    "title": title,
                    "author": author,
                    "url": full_url,
                    "cover_url": self._extract_cover_url(item),
                    "description": self._extract_description(item),
                    "status": self._extract_status(item),
                    "category": self._extract_category(item),
                    "last_updated": self._extract_last_updated(item)
                }

                novels.append(novel_info)

            except Exception:
                continue

        return novels

    def _extract_chapter_list(self, soup, novel_url: str) -> List[Dict[str, Any]]:
        """æå–ç« èŠ‚åˆ—è¡¨"""
        chapters = []

        # æ ¹æ®ç½‘ç«™ç»“æ„ç¼–å†™é€‰æ‹©å™¨
        chapter_links = soup.select("div.chapter-list a")

        for link in chapter_links:
            try:
                title = link.get_text().strip()
                href = link.get("href", "")
                full_url = urllib.parse.urljoin(novel_url, href)

                if self._is_valid_chapter(title, href):
                    chapters.append({
                        "title": title,
                        "url": full_url
                    })

            except Exception:
                continue

        return chapters

    def _extract_chapter_title(self, soup) -> str:
        """æå–ç« èŠ‚æ ‡é¢˜"""
        title_selectors = [
            "h1", "h2", ".chapter-title",
            ".title", "title"
        ]

        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text().strip()
                if title and len(title) > 1:
                    return title

        return "ç« èŠ‚å†…å®¹"

    def _extract_chapter_content(self, soup) -> str:
        """æå–ç« èŠ‚å†…å®¹"""
        # æ ¹æ®ç½‘ç«™ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
        content_selectors = [
            "#content", ".content", ".chapter-content",
            ".read-content", "div[class*='content']"
        ]

        content_elem = None
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                break

        if not content_elem:
            return self.extract_content(soup)

        # ç§»é™¤æ— å…³å…ƒç´ 
        for elem in content_elem(["script", "style", "ins", "iframe"]):
            elem.decompose()

        # è·å–å†…å®¹
        content = content_elem.get_text()
        return content

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    def _extract_author(self, element) -> str:
        """æå–ä½œè€…ä¿¡æ¯"""
        # æ ¹æ®ç½‘ç«™ç»“æ„å®ç°
        return "æœªçŸ¥ä½œè€…"

    def _extract_cover_url(self, element) -> str:
        """æå–å°é¢URL"""
        img = element.find("img")
        if img:
            src = img.get("src") or img.get("data-src")
            if src:
                return urllib.parse.urljoin(self.base_url, src)
        return ""

    def _extract_description(self, element) -> str:
        """æå–ç®€ä»‹"""
        # æ ¹æ®ç½‘ç«™ç»“æ„å®ç°
        return ""

    def _extract_status(self, element) -> str:
        """æå–è¿è½½çŠ¶æ€"""
        text = element.get_text().lower()
        if "å®Œç»“" in text or "å®Œæœ¬" in text:
            return "å®Œç»“"
        elif "è¿è½½" in text:
            return "è¿è½½"
        return "unknown"

    def _extract_category(self, element) -> str:
        """æå–åˆ†ç±»"""
        # æ ¹æ®ç½‘ç«™ç»“æ„å®ç°
        return "unknown"

    def _extract_last_updated(self, element) -> str:
        """æå–æ›´æ–°æ—¶é—´"""
        # æ ¹æ®ç½‘ç«™ç»“æ„å®ç°
        return ""

    def _is_valid_chapter(self, title: str, href: str) -> bool:
        """éªŒè¯ç« èŠ‚é“¾æ¥æœ‰æ•ˆæ€§"""
        if len(title) <= 1 or not href:
            return False

        # è·³è¿‡æ— æ•ˆé“¾æ¥
        skip_patterns = [
            r"javascript:", r"#", r"ç›®å½•", r"ä¹¦ç­¾",
            r"æ”¶è—", r"æ¨è", r"æ’è¡Œ"
        ]

        for pattern in skip_patterns:
            if re.search(pattern, title, re.IGNORECASE):
                return False

        return True

    def _validate_novel_info(self, novel_info: Dict[str, Any]) -> bool:
        """éªŒè¯å°è¯´ä¿¡æ¯"""
        required_fields = ["title", "author", "url"]

        for field in required_fields:
            if not novel_info.get(field) or len(novel_info[field].strip()) < 2:
                return False

        return True

    def _validate_chapter(self, chapter: Dict[str, Any]) -> bool:
        """éªŒè¯ç« èŠ‚ä¿¡æ¯"""
        title = chapter.get("title", "").strip()
        url = chapter.get("url", "").strip()

        return len(title) > 1 and url.startswith("http")

    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡"""
        seen = set()
        unique = []

        for item in results:
            key = (item["title"], item["url"])
            if key not in seen:
                unique.append(item)
                seen.add(key)

        return unique

    def clean_novel_content(self, text: str) -> str:
        """æ¸…ç†å°è¯´å†…å®¹"""
        if not text:
            return ""

        # å…ˆä½¿ç”¨åŸºç±»æ¸…ç†
        text = self.clean_text(text)

        # ç«™ç‚¹ç‰¹å®šæ¸…ç†
        text = re.sub(r'è¯·è®°ä½.*?[wW][wW][wW]\.[^s]+', '', text)
        text = re.sub(r'æœ€æ–°ç« èŠ‚.*?$', '', text)
        text = re.sub(r'\s*\n\s*\n\s*', '\n\n', text)

        return text.strip()


# ä¸ºäº†å‘åå…¼å®¹ï¼Œåˆ›å»ºåˆ«å
{SiteName}Crawler = {SiteName}CrawlerRefactored
```

### ä½¿ç”¨æ¨¡æ¿æ­¥éª¤
1. å¤åˆ¶æ¨¡æ¿ä»£ç 
2. æ›¿æ¢ `{SiteName}` ä¸ºå®é™…ç«™ç‚¹åç§°
3. è°ƒæ•´ `base_url` å’Œ `RequestStrategy`
4. æ ¹æ®ç›®æ ‡ç½‘ç«™ç»“æ„ä¿®æ”¹é€‰æ‹©å™¨
5. å®ç°ç«™ç‚¹ç‰¹å®šçš„è¾…åŠ©æ–¹æ³•
6. åœ¨ `crawler_factory.py` ä¸­æ³¨å†Œæ–°çˆ¬è™«
7. ç¼–å†™å•å…ƒæµ‹è¯•
8. é›†æˆæµ‹è¯•éªŒè¯

## ğŸ“ æ€»ç»“

æœ¬è§„èŒƒå®šä¹‰äº†Novel Builderé¡¹ç›®çˆ¬è™«å¼€å‘çš„æ ‡å‡†æµç¨‹å’Œæœ€ä½³å®è·µã€‚éµå¾ªè¿™äº›è§„èŒƒå¯ä»¥ï¼š

- **ä¿è¯ä»£ç è´¨é‡**: ç»Ÿä¸€çš„ä»£ç é£æ ¼å’Œç»“æ„
- **æé«˜å¼€å‘æ•ˆç‡**: æ ‡å‡†åŒ–çš„å¼€å‘æ¨¡æ¿å’Œå·¥å…·
- **ç¡®ä¿ç³»ç»Ÿç¨³å®š**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæµ‹è¯•è¦†ç›–
- **ä¾¿äºç»´æŠ¤æ‰©å±•**: æ¸…æ™°çš„æ¶æ„å’Œæ–‡æ¡£

å¼€å‘æ–°çˆ¬è™«æ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªæœ¬è§„èŒƒï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚